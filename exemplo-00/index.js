import tf from '@tensorflow/tfjs-node';

async function trainModel(inputXs, outputYs) {
    const model = tf.sequential();

    // primeira camada da rede:
    // Entrada de 7 posições (idade normalizada + 3 cores + 3 localizações)

    // 80 neuronios = aqui coloquei tudo isso por que tem pouca base de treino
    // quanto mais neuronios, mais complexidade a rede pode aprender
    // e consequentemente, mais processamento ela vai usar

    // A relu age como um filtro:
    // como se ela deixasse somente os dados interessantes seguirem viagem na rede
    // Se a informação que chegou nesse neuronio passa para a frente
    // se for 0 ou negativa pode jogar fora, não vai servir para nada
    model.add(tf.layers.dense({ inputShape: [7], units: 80, activation: 'relu' }));

    // saida: 3 neuronios
    // um para cada categoria(premium, medium,basic)
    //activation: normaliza a saida em probabilidades
    model.add(tf.layers.dense({ units: 3, activation: 'softmax' }))

    // Compilando o modelo
    // optimizer Adam(Adaptive Moment Estimation)
    // É um treinador pessoal moderno para redes neurais
    // Ajusta os pesos de forma eficiente e inteligente
    // aprender com historico de erros e acertos
    // loss: categoricalCrossentropy
    // compara oque o modelo "acha"(os scores de cada categoria com a resposta certa)
    // A categoria premium sera sempre [1, 0, 0]
    // Quanto mais distante da previsão do modelo da resposta correta, maior o erro(loss)
    // Exemplo classico: classificação de imagens, recomendação, categorização de usuários
    // Qualquer coisa em que a resposta certa é apenas uma entre varias possiveis
    model.compile({
        optimizer: 'adam',
        loss: 'categoricalCrossentropy',
        metrics: ['accuracy']
    });

    // Treinamento do modelo
    // verbose desabilita o log interno(e usa so o callback)
    // epochs: quantidade de vezes que vai rodar no dataset
    // shuffle: embaralha os dados para evitar víes
    await model.fit(
        inputXs,
        outputYs,
        {
            verbose: 0,
            epochs: 100,
            shuffle: true,
            callbacks: {
                onEpochEnd: (epoch, log) => {
                    `Epoch: ${epoch}: loss = ${log.loss}`
                }
            }
        }
    );

    return model;
}

async function predict(model, pessoa) {
    // transformar o arrayjs para um tensorFlowjs
    const tfInput = tf.tensor2d(pessoa);

    // faz a predição(output sera um tensor de 3 probabilidades)
    const pred = model.predict(tfInput)
    const predArray = await pred.array()
    return predArray[0].map((prob, index) => ({ prob, index }))
}

// Exemplo de pessoas para treino (cada pessoa com idade, cor e localização)
// const pessoas = [
//     { nome: "Erick", idade: 30, cor: "azul", localizacao: "São Paulo" },
//     { nome: "Ana", idade: 25, cor: "vermelho", localizacao: "Rio" },
//     { nome: "Carlos", idade: 40, cor: "verde", localizacao: "Curitiba" }
// ];

// Vetores de entrada com valores já normalizados e one-hot encoded
// Ordem: [idade_normalizada, azul, vermelho, verde, São Paulo, Rio, Curitiba]
// const tensorPessoas = [
//     [0.33, 1, 0, 0, 1, 0, 0], // Erick
//     [0, 0, 1, 0, 0, 1, 0],    // Ana
//     [1, 0, 0, 1, 0, 0, 1]     // Carlos
// ]

// Usamos apenas os dados numéricos, como a rede neural só entende números.
// tensorPessoasNormalizado corresponde ao dataset de entrada do modelo.
const tensorPessoasNormalizado = [
    [0.33, 1, 0, 0, 1, 0, 0], // Erick
    [0, 0, 1, 0, 0, 1, 0],    // Ana
    [1, 0, 0, 1, 0, 0, 1]     // Carlos
]

// Labels das categorias a serem previstas (one-hot encoded)
// [premium, medium, basic]
const labelsNomes = ["premium", "medium", "basic"]; // Ordem dos labels
const tensorLabels = [
    [1, 0, 0], // premium - Erick
    [0, 1, 0], // medium - Ana
    [0, 0, 1]  // basic - Carlos
];

// Criamos tensores de entrada (xs) e saída (ys) para treinar o modelo
const inputXs = tf.tensor2d(tensorPessoasNormalizado)
const outputYs = tf.tensor2d(tensorLabels)

// Quanto mais dados melhor
// Assim o algoritmo consegue entender melhor os padrões complexos dos dados
const model = await trainModel(inputXs, outputYs)

const pessoa = { nome: "zé", idade: 28, cor: "verde", localizacao: "Curitiba" }
// normalizando a idade da nova pessoa, usando o mesmo padrão do treino
// exemplo: idadeMin=25 idadeMax = 40, então = (28-25)/(40-25) = 0.2
const pessoaTersorNormalizado = [
    [
        0.2,//idade normalizada
        1,  //cor azul
        0,  // cor vermelho
        0,  // cor verde
        0,  //são paulo
        1,  // rio
        0   // curitiba
    ],
]
const predictions = await predict(model, pessoaTersorNormalizado)
const results = predictions.sort((a, b) => b.prob - a.prob)
    .map(p => `${labelsNomes[p.index]} (${(p.prob * 100).toFixed(2)}%)`)
    .join('\n')
console.log(results)